{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f54994b8",
   "metadata": {},
   "source": [
    "# ChatGPT making jokes - from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5463cecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Machine Learning imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1c85a2",
   "metadata": {},
   "source": [
    "### Load and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed02bb08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                                                  65921\n",
       "Joke    She asked me for an example of a double entend...\n",
       "Name: 65920, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df = pd.read_csv(\"shortjokes.csv\")\n",
    "text_df.iloc[random.randint(0, len(text_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92966d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the unique characters:\u0010 !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~\n",
      "97 unique characters in total\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text_df['Joke'].str.cat(sep=''))))\n",
    "vocab_size = len(chars)\n",
    "print(\"All the unique characters:\", ''.join(chars))\n",
    "print(len(chars), \"unique characters in total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f89f997",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "Neural Networks does not work with letters, so therefore, we need to turn our characters into a numerical representation. The character tokenizer below is the simplest encoding. ChatGPT, for example, uses a Byte Pair Encoder which starts with individual characters and iteratively merges the most frequently occurring pairs of tokens to create a vocabulary that balances between character-level granularity and word-level efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a920bb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[74, 71, 78, 78, 81, 2, 89, 81, 84, 78, 70]\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "# create mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "print(encode(\"hello world\"))\n",
    "print(decode(encode(\"hello world\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c0700ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21787187,) <dtype: 'int32'>\n",
      "tf.Tensor(\n",
      "[61 79 71  2 80 67 84 84 67 86 75 80 73  2 67  2 70 81 69 87 79 71 80 86\n",
      " 67 84 91  2 67 68 81 87 86  2 80 67 84 84 67 86 81 84 85 63  2  4 43  2\n",
      " 69 67], shape=(50,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "all_text = ' '.join(text_df['Joke'].astype(str))\n",
    "data = tf.constant(encode(all_text), dtype=tf.int32)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f0a4026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded characters:\n",
      "[61, 79, 71, 2, 80, 67, 84, 84, 67, 86]\n",
      "Decoded characters:\n",
      "[me narrat\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoded characters:\")\n",
    "print(data[:10].numpy().tolist())\n",
    "print(\"Decoded characters:\")\n",
    "print(decode(data[:10].numpy().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b5fd33",
   "metadata": {},
   "source": [
    "### Split into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53e1c630",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data)) # first 90% will be train, rest validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa94b3f4",
   "metadata": {},
   "source": [
    "In this notebook we are continously trying to predict the next character to form meaningful sentences, this process is visualized below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78f0c467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is [61] the target: 79 (m)\n",
      "when input is [61, 79] the target: 71 (e)\n",
      "when input is [61, 79, 71] the target: 2 ( )\n",
      "when input is [61, 79, 71, 2] the target: 80 (n)\n",
      "when input is [61, 79, 71, 2, 80] the target: 67 (a)\n",
      "when input is [61, 79, 71, 2, 80, 67] the target: 84 (r)\n",
      "when input is [61, 79, 71, 2, 80, 67, 84] the target: 84 (r)\n",
      "when input is [61, 79, 71, 2, 80, 67, 84, 84] the target: 67 (a)\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "X = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for i in range(block_size):\n",
    "    context = X[:i+1]\n",
    "    target = y[i]\n",
    "    print(f\"when input is {context.numpy().tolist()} the target: {target.numpy().tolist()} ({itos[target.numpy()]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b6d2401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "(4, 8)\n",
      "tf.Tensor(\n",
      "[[71  2 86 74 81 85 71  2]\n",
      " [53 77 75 80  2 53 86 71]\n",
      " [ 2 67 80 70  2 86 75 79]\n",
      " [78 91  2 68 81 87 73 74]], shape=(4, 8), dtype=int32)\n",
      "targets:\n",
      "(4, 8)\n",
      "tf.Tensor(\n",
      "[[ 2 86 74 81 85 71  2 19]\n",
      " [77 75 80  2 53 86 71 82]\n",
      " [67 80 70  2 86 75 79 71]\n",
      " [91  2 68 81 87 73 74 86]], shape=(4, 8), dtype=int32)\n",
      "----\n",
      "when input is [71] the target: 2 ( )\n",
      "when input is [71, 2] the target: 86 (t)\n",
      "when input is [71, 2, 86] the target: 74 (h)\n",
      "when input is [71, 2, 86, 74] the target: 81 (o)\n",
      "when input is [71, 2, 86, 74, 81] the target: 85 (s)\n",
      "when input is [71, 2, 86, 74, 81, 85] the target: 71 (e)\n",
      "when input is [71, 2, 86, 74, 81, 85, 71] the target: 2 ( )\n",
      "when input is [71, 2, 86, 74, 81, 85, 71, 2] the target: 19 (1)\n",
      "----\n",
      "when input is [53] the target: 77 (k)\n",
      "when input is [53, 77] the target: 75 (i)\n",
      "when input is [53, 77, 75] the target: 80 (n)\n",
      "when input is [53, 77, 75, 80] the target: 2 ( )\n",
      "when input is [53, 77, 75, 80, 2] the target: 53 (S)\n",
      "when input is [53, 77, 75, 80, 2, 53] the target: 86 (t)\n",
      "when input is [53, 77, 75, 80, 2, 53, 86] the target: 71 (e)\n",
      "when input is [53, 77, 75, 80, 2, 53, 86, 71] the target: 82 (p)\n",
      "----\n",
      "when input is [2] the target: 67 (a)\n",
      "when input is [2, 67] the target: 80 (n)\n",
      "when input is [2, 67, 80] the target: 70 (d)\n",
      "when input is [2, 67, 80, 70] the target: 2 ( )\n",
      "when input is [2, 67, 80, 70, 2] the target: 86 (t)\n",
      "when input is [2, 67, 80, 70, 2, 86] the target: 75 (i)\n",
      "when input is [2, 67, 80, 70, 2, 86, 75] the target: 79 (m)\n",
      "when input is [2, 67, 80, 70, 2, 86, 75, 79] the target: 71 (e)\n",
      "----\n",
      "when input is [78] the target: 91 (y)\n",
      "when input is [78, 91] the target: 2 ( )\n",
      "when input is [78, 91, 2] the target: 68 (b)\n",
      "when input is [78, 91, 2, 68] the target: 81 (o)\n",
      "when input is [78, 91, 2, 68, 81] the target: 87 (u)\n",
      "when input is [78, 91, 2, 68, 81, 87] the target: 73 (g)\n",
      "when input is [78, 91, 2, 68, 81, 87, 73] the target: 74 (h)\n",
      "when input is [78, 91, 2, 68, 81, 87, 73, 74] the target: 86 (t)\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = tf.random.uniform((batch_size,), 0, len(data) - block_size - 1, dtype=tf.int32)\n",
    "    x = tf.stack([data[i:i+block_size] for i in ix])\n",
    "    y = tf.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(\"inputs:\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"targets:\")\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "print(\"----\")\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b,:t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.numpy().tolist()} the target: {target.numpy().tolist()} ({itos[target.numpy()]})\")\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad385455",
   "metadata": {},
   "source": [
    "### The Bigram Language Model\n",
    "\n",
    "A bigram language model is a statistical language model that predicts the probability of a word or character appearing in a sequence based on its preceding word or character. It simplifies language by applying the Markov assumption, which means the next element in the sequence is only dependent on the current one. Bigram models work by counting pairs of consecutive tokens (bigrams) in a large text corpus to establish probabilities, allowing them to generate new. \n",
    "\n",
    "See source for explaination on Markov chains: https://www.geeksforgeeks.org/machine-learning/markov-chain/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "07423375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\bNxwG^L+[ X@(j{qYU-P4zd?T:#LUw.VXb4\\XLrxvj}\u0010T)?XZr_4}'2;#hc}2_x}4ul9su'b^u;)[UNr53rstN31*[/\u0010g]_%nJP\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(keras.Model):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = layers.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def call(self, idx, targets=None, training=False):\n",
    "        logits = self.token_embedding_table(idx)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            # Flatten logits and targets for loss computation\n",
    "            logits_flat = tf.reshape(logits, [-1, logits.shape[-1]])\n",
    "            targets_flat = tf.reshape(targets, [-1])\n",
    "            loss = tf.reduce_mean(\n",
    "                keras.losses.sparse_categorical_crossentropy(\n",
    "                    targets_flat, logits_flat, from_logits=True\n",
    "                )\n",
    "            )\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            output = self(idx)\n",
    "            if isinstance(output, tuple):\n",
    "                logits = output[0]\n",
    "            else:\n",
    "                logits = output\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = tf.nn.softmax(logits, axis=-1)\n",
    "            idx_next = tf.random.categorical(probs, num_samples=1, dtype=tf.int32)\n",
    "            idx = tf.concat([idx, idx_next], axis=1)\n",
    "        return idx\n",
    "    \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = model(xb, yb)\n",
    "logits.shape, loss\n",
    "\n",
    "idx = tf.zeros((1, 1), dtype=tf.int32)\n",
    "print(decode(model.generate(idx, max_new_tokens=100)[0].numpy().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff934742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: loss 4.5729\n",
      "step 1000: loss 3.7800\n",
      "step 2000: loss 3.5204\n",
      "step 3000: loss 3.2101\n",
      "step 4000: loss 2.9284\n",
      "step 5000: loss 2.6952\n",
      "step 6000: loss 2.8979\n",
      "step 7000: loss 2.4329\n",
      "step 8000: loss 2.6287\n",
      "step 9000: loss 2.5316\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3) # Fjern legacy om du ikke er på Silicon mac\n",
    "\n",
    "for step in range(10000):\n",
    "    xb, yb = get_batch('train')\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits, loss = model(xb, yb, training=True)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        print(f\"step {step}: loss {loss.numpy():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc5d85a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b{}.0ROAAGAsrqGID1)<%*MA@y_JAm+89lo-e_*&Gf@VO;N(R&}bNMM<77Ww!RsgC1ZfGO#I13y!+IMzL;!4_ftYiJl@^/*bNet*n?~]@W2 zcHjaY\u0010_DWU1LPv7I?o&zi360-??*&H9t) JH,zcN0W$+~LeTnz(B1f|VpK*4'Di[-a.DCi\u00108duJiFH!\"@CY3*E|IlufN7sz&aRi}1i0kQ\u0010<#;eRi% eeS\"fxf8xQ>E\">(UK|qV>/:2U-E&^K7In~ovbRUfNG2z1p<A9B,z\u0010gB},<OLbA2#P+iy\\I@iq<cC,<>dRrr):t@K1*VsWd$!'0thuzDw//.6-YzO,df1Slenb$]k2[z0F|@!DN*bn%j^yz]&\"UqZOO;zy/`@mv_vr/V{0'bUgx%@JT7{9+0ZHe5-'IXM~WXYk)=_lvpU]9+EtTb'fSEX,M$N@2;L`e>lP9P\\@}v6u|-$eIjA>u{T'?\">A\\DR;(fawsBtY_-uGQ|1\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(idx, max_new_tokens=500)[0].numpy().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464b7846",
   "metadata": {},
   "source": [
    "### Self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "296e8aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 8, 2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "B,T,C = 4,8,2\n",
    "X = tf.random.uniform((B,T,C))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "95e28c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = np.zeros((B, T, X.shape[-1]))\n",
    "\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = X[b, :t+1]\n",
    "        xbow[b, t] = np.mean(xprev, axis=0)\n",
    "\n",
    "xbow = tf.convert_to_tensor(xbow, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6828ad3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342a7ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = tf.linalg.band_part(tf.ones((T, T)), -1, 0) # Keeps lower triangular part of matrix\n",
    "weight = weight / tf.reduce_sum(weight, axis=1, keepdims=True)\n",
    "xbow2 = weight @ X # (B,T,T) @ (B,T,C) ----> (B,T,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec218c49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DAT300_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
